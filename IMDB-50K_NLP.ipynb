{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLassign2_NLP.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y_eQ3Oz0LQU"
      },
      "source": [
        "# Question No.2. NLP Dataset\n",
        "A python notebookto build, train and evaluate a deep neural network on the IMDB-50K dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOxzSgJ70NMG"
      },
      "source": [
        "# **1. Import Libraries/Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORhQdMie0kJK"
      },
      "source": [
        "1a.Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l-IBTRU0Ovb"
      },
      "source": [
        "#importing required libraries\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from sklearn import metrics  \n",
        "import time\n",
        "\n",
        "import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense, Dropout, Conv1D, MaxPooling1D, LSTM\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "import helper\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtRm4CHW0goh"
      },
      "source": [
        "1b. Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cycXrbMNuqSh"
      },
      "source": [
        "train_data ,test_data  = tfds.load(name=\"imdb_reviews\", split=['train', 'test'],  \n",
        "                                  batch_size=-1, as_supervised=True)\n",
        "\n",
        "train_examples, train_labels = tfds.as_numpy(train_data)\n",
        "test_examples, test_labels = tfds.as_numpy(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Vli1pD1S-B"
      },
      "source": [
        "1c. Check the GPU available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQZSkM-1UH_"
      },
      "source": [
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BrL2Mhr2QbD"
      },
      "source": [
        "# **2. Data Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39plOaR22U7s"
      },
      "source": [
        "print(\"Categories:\", np.unique(train_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glsXNIfF3729"
      },
      "source": [
        "Output above that the dataset is labeled into two categories,  0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyARqjV-xiYk"
      },
      "source": [
        "length = [len(i) for i in train_examples]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXD90ZZGxomF"
      },
      "source": [
        "The average review length of review in training example is 1325 words, with a standard deviation of 1003 words. ( Huge difference between the length of the reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTIRXSam40kz"
      },
      "source": [
        "**2a** Print at least two movie reviews from each class of the dataset, for a sanity check that labels match\n",
        "the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xfGIgbB4CJV"
      },
      "source": [
        "#Printing first 10 examples\n",
        "train_examples[:10]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj6kG-vTit30"
      },
      "source": [
        "Checking if the review matches with the label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFiTYuQBjWKT"
      },
      "source": [
        "train_labels[:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUisNeWExKLh"
      },
      "source": [
        "0 indicates negative sentiment where as 1 indicates positive sentiment. The reviews are matching with the sentiment as indicated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG4f9NofxrVR"
      },
      "source": [
        "#Number of training and test entries\n",
        "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r5CKcFhqfc1"
      },
      "source": [
        "The Training and test data is 50/50. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX3OBWdrlLcQ"
      },
      "source": [
        "2b. Plot a bar graph of class distribution in dataset. Each bar depicts the number of reviews belonging to a particular sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb8z8SH0lVB8"
      },
      "source": [
        "fig, axs = plt.subplots(1,2,figsize=(10,5)) \n",
        "# Count plot for training set\n",
        "sns.countplot(train_labels.ravel(), ax=axs[0])\n",
        "axs[0].set_title('Distribution of Training data Lables')\n",
        "axs[0].set_xlabel('Classes')\n",
        "# Count plot for testing set\n",
        "sns.countplot(test_labels.ravel(), ax=axs[1])\n",
        "axs[1].set_title('Distribution of Testing data Lables')\n",
        "axs[1].set_xlabel('Classes')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBZVcekUZPyB"
      },
      "source": [
        "print(\"X_train shape is : \", train_examples.shape)\n",
        "print(\"X_test shape  is : \", train_labels.shape)\n",
        "print(\"y_train shape is : \", test_examples.shape)\n",
        "print(\"y_test shape is : \", test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ned6o1HYwCzx"
      },
      "source": [
        "# **3. Data Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OISJvtG3wX9j"
      },
      "source": [
        "The reviews—the arrays of strings—must be converted to into embeddings vectors before fed into the neural network.We can use a pre-trained text embedding as the first layer for text pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjCB7QcMzNRx"
      },
      "source": [
        "3.b. We will use a model from TensorFlow Hub called google/tf2-preview/nnlm-en-dim128/1 for the same[ ~1M vocabulary size and 128 dimensions]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVkwoTdXwGaf"
      },
      "source": [
        "# Creating a Keras layer that uses a TensorFlow Hub model to embed the sentences\n",
        "model = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
        "hub_layer = hub.KerasLayer(model, output_shape=[128], input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmMhF27oSqzA"
      },
      "source": [
        "# **4. Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zpUyN_cStw6"
      },
      "source": [
        "a. Add dense layers, specifying the number of units in each layer and the activation function used in the layer.\n",
        "\n",
        "b. Add L2 regularization to all the layers.\n",
        "\n",
        "c. Add one layer of dropout at the appropriate position and give reasons.\n",
        "\n",
        "d. Choose the appropriate activation function for all the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4BQYVH2TA4p"
      },
      "source": [
        "#Model Configurations\n",
        "\n",
        "# Create a model object\n",
        "SeqModel = tf.keras.Sequential()\n",
        "\n",
        "# Layer 1 = input layer - TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector.\n",
        "SeqModel.add(hub_layer)\n",
        "\n",
        "\n",
        "# Layer 2 = hidden layer \n",
        "SeqModel.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
        "\n",
        "\n",
        "# Add dropout of 30% to layer 3\n",
        "SeqModel.add(layers.Dropout(0.3))\n",
        "\n",
        "# Layer 3 = hidden layer \n",
        "SeqModel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\n",
        "\n",
        "#SeqModel.add(layers.Dropout(0.3))\n",
        "\n",
        "# Layer 4 = output layer\n",
        "SeqModel.add(Dense(1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR3iDbhU-Ywj"
      },
      "source": [
        "e. Print the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EimuPEQ4-er0"
      },
      "source": [
        "SeqModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOMyPhn_LcLq"
      },
      "source": [
        "# **5.Model Compilation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7ExzXQxLhd_"
      },
      "source": [
        "5a. Compile the model with the appropriate loss function.\n",
        "\n",
        "5b. Use an appropriate optimizer. Give reasons for the choice of learning rate and its value.\n",
        "\n",
        "5c. Use accuracy as metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4wlGUbP9q7q"
      },
      "source": [
        "def ModelCompile (optimi,loss):\n",
        "  SeqModel.compile(optimizer=optimi,loss=loss,metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1nbwbeF8bwN"
      },
      "source": [
        "optimizer = 'adam'\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "ModelCompile(optimizer,bce)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMoAT81iTvVu"
      },
      "source": [
        "# **6. MODEL TRAINING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5BjA4fCTzxb"
      },
      "source": [
        "6a. Train the model for an appropriate number of epochs (print the train and validation accuracy/loss for each epoch). Use the appropriate batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ltgHMw9zE3"
      },
      "source": [
        "x_val = train_examples[:10000]\n",
        "partial_x_train = train_examples[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofM9HCd-T_C0"
      },
      "source": [
        "#Train the model\n",
        "#Prints btotal time taken for training\n",
        "def trainmodel(Model):\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "  mlp_start = time.time()\n",
        "  history = Model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=2)\n",
        "  mlp_end = time.time()\n",
        "  mlp_took = mlp_end -mlp_start\n",
        "  print(\"Total time taken: %s seconds\"%(mlp_took))\n",
        "  return(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgwI1sG19JnT"
      },
      "source": [
        "h = trainmodel(SeqModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82HYVDD_LcVu"
      },
      "source": [
        "# **7. Model Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgN2fed-XLQz"
      },
      "source": [
        "7a. Print the final test/validation loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5twjSpgaldw"
      },
      "source": [
        "# Test the model \n",
        "def TestPrintResults (data, labels, Model):\n",
        "  test_results = Model.evaluate(data, labels, verbose=False)\n",
        "  print(f'Results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boHmXD0ei7Tv"
      },
      "source": [
        "#Test the model after training on the test dataset\n",
        "print(\"Test \")\n",
        "TestPrintResults(test_data,test_labels,SeqModel)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kHkvcwnqfm4"
      },
      "source": [
        "#Test the model after training on the validation dataset\n",
        "print(\"Validation \")\n",
        "TestPrintResults(x_val,y_val,SeqModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egg1gd7KUE4r"
      },
      "source": [
        "# plot training history\n",
        "def plotLosses(h): \n",
        "  plt.plot(h.history['accuracy'], label='train')\n",
        "  plt.plot(h.history['val_accuracy'], label='validation')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Plot of Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(h.history['loss'], label='train')\n",
        "  plt.plot(h.history['val_loss'], label='validation')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Plot of Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gniNhsZdEn5V"
      },
      "source": [
        "plotLosses(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px4pIjSnXIZ-"
      },
      "source": [
        "7b. Print confusion matrix and classification report for the validation dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Igb_p45XSMU"
      },
      "source": [
        "# Print Confusion Matrix\n",
        "def printconfusionmatrix(Model):\n",
        "  predictions = Model.predict_classes(test_data,verbose=0)\n",
        "  cm = confusion_matrix(test_labels, predictions)\n",
        "  print('Confusion matrix: \\n \\n', cm)\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHdcqWCucuDp"
      },
      "source": [
        "#Classification report for bag of words \n",
        "def classificationreport(predictions):\n",
        "  creport=classification_report(test_labels,predictions,target_names=['Positive','Negative'])\n",
        "  print(\"\\n Classification Report \\n\",creport)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNr2ApYFrk2v"
      },
      "source": [
        "predictions = printconfusionmatrix(SeqModel)\n",
        "classificationreport(predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWmm6Ki6mpAy"
      },
      "source": [
        "Summary for the\n",
        "best and worst performing class and the overall trend\n",
        "The first row of the confusion matrix is for reviews which their actual sentiment values in the test set are 1. As you can calculate, out of 25,000 reviews, the sentiment value of 12,500 of them is 1, the classifier correctly predicted 10976 of them as 1.\n",
        "It means, for 10976 reviews, the actual sentiment values were 1 in the test set, and the classifier also correctly predicted those as 1. However, while the actual labels of 1524 reviews were 1, the classifier predicted those as 0.\n",
        "\n",
        "The second row of the confusion matrix is for reviews which their actual sentiment values in the test set are 0. As you can calculate, out of 25,000 reviews, the sentiment value of 12,500 of them is 0, the classifier correctly predicted 10333 of them as 0.\n",
        "It means, for 10333 reviews, the actual sentiment values were 0 in the test set, and the classifier also correctly predicted those as 0. However, while the actual labels of 2167 reviews were 0, the classifier predicted those as 1. Comparitively the sentiment for negative review is difficult to predict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zru7iybZeia7"
      },
      "source": [
        "# **Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDst7AGlelht"
      },
      "source": [
        "1. Network Depth: Change the number of hidden layers and hidden units for each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XqFg_BUetUa"
      },
      "source": [
        "\n",
        "# Create a model object\n",
        "SeqModel2 = tf.keras.Sequential()\n",
        "\n",
        "# Layer 1 = input layer - TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector.\n",
        "SeqModel2.add(hub_layer)\n",
        "SeqModel2.add(layers.Dropout(0.3))\n",
        "\n",
        "# Layer 2 = hidden layer \n",
        "#SeqModel2.add(Dense(32,kernel_regularizer=regularizers.l2(0.005)))\n",
        "\n",
        "SeqModel2.add(Dense(1,kernel_regularizer=regularizers.l2(0.005)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz6ryh3kflDX"
      },
      "source": [
        "SeqModel2.compile(optimizer=\"adam\",loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n",
        "h1 = trainmodel(SeqModel2)\n",
        "plotLosses(h1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyR-1y_9hcmP"
      },
      "source": [
        "#Test the model after training on the Test dataset\n",
        "print(\"Test \")\n",
        "TestPrintResults(test_data,test_labels,SeqModel2)\n",
        "#Test the model after training on the validation dataset\n",
        "print(\"\\nValidation \")\n",
        "TestPrintResults(x_val,y_val,SeqModel2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DZuF2PC48uz"
      },
      "source": [
        "predictions = printconfusionmatrix(SeqModel2)\n",
        "classificationreport(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3d2UD-F46Q3"
      },
      "source": [
        "8.2. Regularization: Train a model without regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XakLwNAIh-7-"
      },
      "source": [
        "# Create a model object\n",
        "SeqModel3 = tf.keras.Sequential()\n",
        "\n",
        "# Layer 1 = input layer - TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector.\n",
        "SeqModel3.add(hub_layer)\n",
        "\n",
        "\n",
        "# Layer 2 = hidden layer \n",
        "SeqModel3.add(Dense(1, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ash1KT4hiHp2"
      },
      "source": [
        "SeqModel3.compile(optimizer=\"adam\",loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])\n",
        "h3 = trainmodel(SeqModel3)\n",
        "plotLosses(h3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fftKyOOhiKLm"
      },
      "source": [
        "#Test the model after training on the Test dataset\n",
        "print(\"Test \")\n",
        "TestPrintResults(test_data,test_labels,SeqModel3)\n",
        "#Test the model after training on the validation dataset\n",
        "print(\"\\nValidation \")\n",
        "TestPrintResults(x_val,y_val,SeqModel3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9lw0x_CiMWz"
      },
      "source": [
        "predictions = printconfusionmatrix(SeqModel3)\n",
        "classificationreport(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhdkq-NfpRNO"
      },
      "source": [
        "We can see that there is no much difference in the accuracy of SeqModel and SeqModel2 which has lesser number of layers. The model is able to predict with the same accuracy with lesser number or layers as well . Also the amount of time spent to learn the model decreases as the number of layers decreases.\n",
        "\n",
        "\n",
        "Doing regularization prevents Overfitting as seen from the plots as indicated above also the model is trained on a lower dimensional dataset is computationally efficient. "
      ]
    }
  ]
}